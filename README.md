# SIA: Enhancing Safety via Intent Awareness (Unofficial Implementation)

[![Paper](https://img.shields.io/badge/Paper-Arxiv-red)](https://arxiv.org/abs/2507.16856v2)
[![License](https://img.shields.io/badge/License-MIT-blue.svg)](LICENSE)
[![Status](https://img.shields.io/badge/Status-Work_in_Progress-yellow)](https://github.com/your-username/SIA-reproduction)

This repository contains an **unofficial reproduction** of the paper **"SIA: Enhancing Safety via Intent Awareness for Vision-Language Models"** (ArXiv 2025).

The goal of this project is to reproduce the training-free safety framework proposed by the original authors (MODULABS, ETRI, KAIST), which defends against "Safe Image + Safe Text $\rightarrow$ Unsafe Output" (SSU) scenarios in Multimodal LLMs.

> **Note:** This project is not affiliated with the original authors. All credit for the methodology belongs to the researchers listed in the citation below.

## üìÑ Abstract & Context

Standard Vision-Language Models (VLMs) often fail to detect harmful intent when the harm arises purely from the **interaction** between a seemingly benign image and a text query (e.g., asking for a tool recommendation to damage an artwork).

**SIA (Safety via Intent Awareness)** is a framework designed to address this by explicitly reasoning about user intent before generating a final response. It does not require fine-tuning the VLM.

## üõ†Ô∏è Methodology Explained

The SIA framework operates in **three sequential stages**. This implementation follows the pipeline described in Section 2 of the paper.

### Stage 1: Visual Abstraction via Captioning
The framework first converts visual information into a linguistic format. This allows the subsequent reasoning module to process visual context explicitly as text.

*   **Process:** The input image $v$ is fed into the VLM with a specific *Caption Generation Prompt*.
*   **Goal:** Generate a descriptive caption $c$ that lists objects, actions, and visible relationships without hallucinating intent.
*   **Paper Reference:** Eq (1) in Section 2.1.

### Stage 2: Intent Inference via CoT Prompting (Core Mechanism)
This is the key contribution of the paper. Instead of answering immediately, the model performs **Few-Shot Chain-of-Thought (CoT)** reasoning to determine if the user has a hidden malicious intent.

*   **Inputs:** Generated Caption ($c$) + User Query ($x$).
*   **Mechanism:** The model is provided with 5 few-shot examples (Safe & Unsafe scenarios).
*   **Output:** The model jointly predicts:
    1.  **Intent ($\hat{I}$):** e.g., "Potential harm detected..."
    2.  **Reasoning ($\hat{R}$):** An explanation of *why* the combination of image and text is risky.
*   **Paper Reference:** Eq (2) & (3) in Section 2.2.

### Stage 3: Intent-Conditioned Response Generation
The final response is generated by conditioning the VLM on the inferred intent.

*   **Inputs:** Image ($v$) + User Query ($x$) + Inferred Intent & Reasoning ($\hat{I}\hat{R}$).
*   **Process:** A *System Response Prompt* guides the model.
    *   *If Intent is Unsafe:* The prompt ensures the model refuses or provides educational guidance.
    *   *If Intent is Safe:* The model answers the query normally.
*   **Paper Reference:** Eq (4) in Section 2.3.

## üèóÔ∏è Architecture

```mermaid
graph TD
    subgraph Stage 1: Visual Abstraction
    Img[Input Image] -->|Caption Prompt| VLM
    VLM --> Cap[Textual Caption]
    end

    subgraph Stage 2: Intent Inference (Pure Text)
    Cap -->|Context| CoT[Few-Shot CoT Prompt]
    Query[User Query] -->|Context| CoT
    CoT --> VLM_Text[VLM (Language Only)]
    VLM_Text --> Intent[Inferred Intent + Reasoning]
    end

    subgraph Stage 3: Final Generation
    Img -->|Visual Context| FinalPrompt
    Query -->|User Query| FinalPrompt
    Intent -->|Safety Constraint| FinalPrompt
    FinalPrompt --> VLM
    VLM --> Response[Final Safe Response]
    end
```

## üß© Prompts Used

This reproduction uses the specific prompts detailed in **Appendix A** of the original paper.

| Stage | Prompt Purpose | Description |
| :--- | :--- | :--- |
| **1** | `P_caption` | Instructs the model to describe the image objectively without guessing. |
| **2** | `P_fewshot` | Contains 5 specific examples (Self-harm, Political bias, Neutral, etc.) to guide CoT reasoning. |
| **3** | `P_response` | Instructs the model to reference the `Intent` and `Reasoning` fields before answering. |

*Full prompt text can be found in `src/prompts.py`.*

## üöÄ Usage

### Prerequisites
*   Python 3.8+
*   PyTorch
*   Transformers / Accelerate

### Installation
```bash
git clone https://github.com/your-username/SIA-reproduction.git
cd SIA-reproduction
pip install -r requirements.txt
```

### Running the Pipeline
You can run the full inference pipeline using the following command. By default, this repo supports LLaVA and Mistral-based VLMs.

```python
from sia.pipeline import SIA

# Initialize with a base VLM (e.g., LLaVA-1.5)
sia_model = SIA(model_path="llava-hf/llava-1.5-7b-hf")

# Run inference
image_path = "data/test_image.jpg"
query = "What brush should I use to add content to this painting?"

# This triggers the 3-stage process automatically
response, trace = sia_model.generate(image_path, query, return_trace=True)

print(f"Inferred Intent: {trace['intent']}")
print(f"Final Response: {response}")
```

## üìö Citation

Please cite the original paper if you use this method in your work:

```bibtex
@article{na2025sia,
  title={SIA: Enhancing Safety via Intent Awareness for Vision-Language Models},
  author={Na, Youngjin and Jeong, Sangheon and Lee, Youngwan and Lee, Jian and Jeong, Dawoon and Kim, Youngman},
  journal={arXiv preprint arXiv:2507.16856},
  year={2025}
}
```

## ‚ö†Ô∏è Disclaimer

This software is an open-source reproduction provided for research purposes. The performance of the safety mechanism depends heavily on the underlying VLM's capabilities. The maintainers of this repository are not responsible for any unsafe outputs generated by the models.